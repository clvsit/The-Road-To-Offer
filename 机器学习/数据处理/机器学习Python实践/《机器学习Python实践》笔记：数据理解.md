# 数据理解
> 定量研究最重要的是如何提出问题，而不是数据和统计方法。定量研究是一个全面的过程，不只是数据，也不只是统计，而是运用统计来研究数据，来回答我们想要了解的问题，数据、统计方法、理念，三者缺一不可，其中，理念是最重要的。每一个研究问题涉及的领域很多，人不能画地为牢，而是要看研究的问题把你带到哪里；定量研究不是数据导向，不是方法导向，也不是学科导向，我比较提倡的是问题导向，提出一个问题，最终目的是回答这个问题。—— 谢宇

> 统计学是什么？概率与数学。用概率与数学来分析人，分析的永远不是人。用永远不是人的结论指导人实在是一种偏误。在这个意义上讲，解读强于技术。理解人比什么都重要，方法、技术、手段、学科都仅仅是背景。定量研究是分析趋势，是找寻和社会相适应的一个概括视角，这才是定量研究。如果你想把数据做好，先要理解社会、理解人、理解营销，才能谈得上定量。—— 刘德寰

## 数据导入
在训练机器学习的模型时，需要用到大量数据，最常用的做法是利用历史的数据来训练模型。这些数据通常会以 CSV 的格式来存储，或者能够方便地转化为 CSV 格式。在开始启动机器学习项目之前，必须先将数据导入到 Python 中。下面将介绍三种将 CSV 数据导入到 Python 中的方法，以便完成对机器学习算法的训练。
- 通过标准的 Python 库导入 CSV 文件。
- 通过 NumPy 导入 CSV 文件。
- 通过 Pandas 导入 CSV 文件。

### CSV 文件
CSV 文件是用逗号（，）分隔的文本文件。在数据导入之前，通常会审查一下 CSV 文件中包含的内容。在审查 CSV 文件时，通常要注意以下几个方面。

#### 文件头
如果 CSV 的文件里包括文件头的信息，可以很方便地使用文件头信息来设置读入数据字段的属性名称。如果文件里不含有文件头信息，需要自己手动设定读入文件的字段属性名称。数据导入时，设置字段属性名称，有助于提高数据处理程序的可读性。

#### 文件中的注释
在 CSV 文件中，注释行是以“井”号（#）开头的。是否需要对读入的注释行做处理，取决于采用什么方式读入 CSV 文件。

#### 分隔符
CSV 文件的标准分隔符是逗号（，），当然也可以使用 Tab 键或空格键作为自定义的分隔符。当使用这两种分隔符时，文件读取是要指明分隔符的。

#### 引号
当有的字段值中有空白时，这些值通常都会被引号引起来，默认使用双引号来标记这些字段值。如果采用自定义格式，那么在文件读取时要明确在文件中采用的自定义格式。

### Pima Indians 数据集
目前在 UCI 机器学习仓库（http://archive.ics.uci.edu/ml/datasets.html）中有大量的免费数据，可以利用这些数据来学习机器学习，并训练算法模型。

Pima Indians 是一个分类问题的数据集，主要记录印第安人最近五年内是否患糖尿病的医疗数据。这些数据都是以数字的方式记录的，并且输出结果是 0 或 1，使我们在机器学习的算法中建立模型变得非常方便。

### 采用标准 Python 类库导入数据
Python 提供了一个标准类库 CSV，用来处理 CSV 文件。这个类库中的 reader() 函数用来读入 CSV 文件。当 CSV 文件被读入后，可以利用这些数据生成一个 NumPy 数组，用来训练算法模型。首先下载数据文件到应用目录下，并命名文件为 pima.csv。这个文件中所有的数据都是数字，并且数据中不含有文件头。

```python
from csv import reader
import numpy as np
# 使用标准的 Python 类库导入 CSV 数据
filename = "./data/pima_data.csv"
with open(filename, "rt") as raw_data:
    readers = reader(raw_data, delimiter=",")
    x = list(readers)
    data = np.array(x).astype("float")
    print(data.shape)
    
# 输出
(768, 9)
```

### 采用 NumPy 导入数据
也可以使用 NumPy 的 loadtxt() 函数导入数据。使用该函数处理的数据没有文件头，并且所有的数据结构是一样的，也就是说数据类型是一样的。

```python
from numpy import loadtxt
# 使用 NumPy 导入 CSV 数据
filename = "./data/pima_data.csv"
with open(filename, "rt") as raw_data:
    data = loadtxt(raw_data, delimiter=",")
    print(data.shape)

# 输出
(768, 9)
```

### 采用 Pandas 导入数据
可以通过使用 pandas.read_csv() 函数来导入 CSV 文件。该函数的返回值是 DataFrame，可以很方便地进行下一步的处理。同时，该函数的名称非常直观，便于代码的阅读和后续对数据的处理。在机器学习的项目中，经常利用 Pandas 来做数据清洗与数据准备工作。因此，在导入 CSV 文件时，推荐使用该方法。

```python
from pandas import read_csv
# 使用 Pandas 导入 CSV 数据
filename = "./data/pima_data.csv"
names = ["preg", "plas", "pres", "skin", "test", "mass", "pedi", "age", "class"]
data = read_csv(filename, names=names)
print(data.shape)

# 输出
(768, 9)
```


## 数据理解
为了得到更准确的结果，必须理解数据的特征、分布情况，以及需要解决的问题，以便建立和优化算法模型。本章将介绍七种方法来帮助大家理解数据。
- 简单地查看数据。
- 审查数据的维度。
- 审查数据的类型和属性。
- 总结查看数据分类的分布情况。
- 通过描述性统计分析数据。
- 理解数据属性的相关性。
- 审查数据的分布状况。

```python
# 首先导入数据
from pandas import read_csv
filename = "./data/pima_data.csv"
names = ["preg", "plas", "pres", "skin", "test", "mass", "pedi", "age", "class"]
data = read_csv(filename, names=names)
```

### 简单地查看数据
对数据的简单审视，是加强对数据理解最有效的方法之一。通过对数据的观察，可以发现数据的内在关系。这些发现有助于对数据进行整理。接下来通过一个简单的例子展示一下如何查看数据。

```python
peek = data.head(10)
print(peek)
```

       preg  plas  pres  skin  test  mass   pedi  age  class
    0     6   148    72    35     0  33.6  0.627   50      1
    1     1    85    66    29     0  26.6  0.351   31      0
    2     8   183    64     0     0  23.3  0.672   32      1
    3     1    89    66    23    94  28.1  0.167   21      0
    4     0   137    40    35   168  43.1  2.288   33      1
    5     5   116    74     0     0  25.6  0.201   30      0
    6     3    78    50    32    88  31.0  0.248   26      1
    7    10   115     0     0     0  35.3  0.134   29      0
    8     2   197    70    45   543  30.5  0.158   53      1
    9     8   125    96     0     0   0.0  0.232   54      1
    

### 数据的维度
在机器学习中要注意数据的行和列，必须对所拥有的数据非常了解，要知道有多少行和多少列，这是因为：
- 太多的行会导致花费大量时间来训练算法得到模型；太少的数据会导致对算法的训练不充分，得不到合适的模型。
- 如果数据具有太多的特征，会引起某些算法性能低下的问题。

通过 DataFrame 的 shape 属性，可以很方便地查看数据集中有多少行和多少列。

```python
print(data.shape)
```

    (768, 9)

### 数据属性和类型
数据的类型是很重要的一个属性。字符串会被转化成浮点数或整数，以便于计算和分类。可以通过 DataFrame 的 Type 属性来查看每一个字段的数据类型。

```python
print(data.dtypes)
```

    preg       int64
    plas       int64
    pres       int64
    skin       int64
    test       int64
    mass     float64
    pedi     float64
    age        int64
    class      int64
    dtype: object
    
### 描述性统计
描述性统计可以给出一个更加直观、更加清晰的视角，以加强对数据的理解。在这里可以通过 DataFrame 的 describe() 方法来查看描述性统计的内容。这个方法给我们展示了八方面的信息：
- 数据记录数
- 平均值
- 标准方差
- 最小值
- 下四分位数
- 中位数
- 上四分位数
- 最大值

这些信息主要用来描述数据的分布情况。

```python
from pandas import set_option
# 描述性统计
set_option('display.width', 100)
# 设置数据的精确度
set_option('precision', 4)
print(data.describe())
```

               preg      plas      pres      skin      test      mass      pedi       age    class
    count  768.0000  768.0000  768.0000  768.0000  768.0000  768.0000  768.0000  768.0000  768.000
    mean     3.8451  120.8945   69.1055   20.5365   79.7995   31.9926    0.4719   33.2409    0.349
    std      3.3696   31.9726   19.3558   15.9522  115.2440    7.8842    0.3313   11.7602    0.477
    min      0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0780   21.0000    0.000
    25%      1.0000   99.0000   62.0000    0.0000    0.0000   27.3000    0.2437   24.0000    0.000
    50%      3.0000  117.0000   72.0000   23.0000   30.5000   32.0000    0.3725   29.0000    0.000
    75%      6.0000  140.2500   80.0000   32.0000  127.2500   36.6000    0.6262   41.0000    1.000
    max     17.0000  199.0000  122.0000   99.0000  846.0000   67.1000    2.4200   81.0000    1.000

### 数据分组分布（适用于分类算法）
在分类算法中，需要知道每个分类的数据大概有多少条记录，以及数据分布是否平衡。如果数据分布的平衡性很差，需要在数据加工阶段进行数据处理，来提高数据分布的平衡性。利用 Pandas 的属性和方法，可以很方便地查看数据的分布情况。

```python
print(data.groupby("class").size())
```

    class
    0    500
    1    268
    dtype: int64
    
### 数据属性的相关性
数据属性的相关性是指数据的两个属性是否互相影响，以及这种影响是什么方式的。非常通用的计算两个属性的相关性的方法是皮尔逊相关系数，皮尔逊相关系数是度量两个变量间相关程度的方法。它是一个介于 1 和 -1 之间的值，其中，1 表示变量完全正相关，0 表示无关，-1 表示完全负相关。在自然科学领域中，该系数广泛用于度量两个变量之间的相关程度。在机器学习中，当数据的关联性比较高时，有些算法（如 linear、逻辑回归算法等）的性能会降低。所以在开始训练算法前，查看一下算法的关联性是一个很好的方法。当数据特征的相关性比较高时，应该考虑对特征进行降维处理。下面通过使用 DataFrame 的 corr() 方法来计算数据集中数据属性之间的关联关系矩阵。

```python
set_option("precision", 2)
print(data.corr(method="pearson"))
```

           preg  plas  pres  skin  test  mass  pedi   age  class
    preg   1.00  0.13  0.14 -0.08 -0.07  0.02 -0.03  0.54   0.22
    plas   0.13  1.00  0.15  0.06  0.33  0.22  0.14  0.26   0.47
    pres   0.14  0.15  1.00  0.21  0.09  0.28  0.04  0.24   0.07
    skin  -0.08  0.06  0.21  1.00  0.44  0.39  0.18 -0.11   0.07
    test  -0.07  0.33  0.09  0.44  1.00  0.20  0.19 -0.04   0.13
    mass   0.02  0.22  0.28  0.39  0.20  1.00  0.14  0.04   0.29
    pedi  -0.03  0.14  0.04  0.18  0.19  0.14  1.00  0.03   0.17
    age    0.54  0.26  0.24 -0.11 -0.04  0.04  0.03  1.00   0.24
    class  0.22  0.47  0.07  0.07  0.13  0.29  0.17  0.24   1.00
    
### 数据的分布分析
通过分析数据的高斯分布情况来确认数据的偏离情况。高斯分布又叫正态分布，是在数据、物理及工程等领域都非常重要的概率分布，在统计学的许多方面有着重大的影响。高斯分布的曲线呈钟形，两头低，中间高，左右对称。在高斯分布图中，y 轴两点之间的面积是发生的概率。在很多机器学习算法中都会假定数据遵循高斯分布，先计算数据的高斯偏离状况，再根据偏离状况准备数据。我们可以使用 DataFrame 的 skew() 方法来计算所有数据属性的高斯分布偏离情况。

```python
print(data.skew())
```

    preg     0.90
    plas     0.17
    pres    -1.84
    skin     0.11
    test     2.27
    mass    -0.43
    pedi     1.92
    age      1.13
    class    0.64
    dtype: float64
    
skew()函数的结果显示了数据分布是左偏还是右偏。当数据接近 0 时，表示数据的偏差非常小。

## 数据可视化
为了生成最优化的算法模型，必须对数据进行理解。最快、最有效的方式是通过数据的可视化来加强对数据的理解。


```python
from pandas import read_csv
import matplotlib.pyplot as plt
filename = "./data/pima_data.csv"
names = ["preg", "plas", "pres", "skin", "test", "mass", "pedi", "age", "class"]
data = read_csv(filename, names=names)
```

### 单一图表
本章将通过以下三种图表来展示数据：
- 直方图
- 密度图
- 箱线图

#### 直方图
直方图（Histogram）又称质量分布图，是一种统计报告图，由一系列高度不等的纵向条纹或线段表示数据的分布情况。一般用横轴表示数据类型，纵轴表示分布情况。直方图可以非常直观地展示每个属性的分布状况。通过图表可以很直观地看到数据是高斯分布，指数分布还是偏态分布。

```python
data.hist()
plt.show()
```

![直方图示例](https://i.imgur.com/blA5HzN.png)

执行结果如上图所示，可以看到，age、pedi 和 test 也许是指数分布；mass、pres 和 plas 也许是高斯分布。

#### 密度图
密度图是一种表现与数据值对应的边界或域对象的图形表示方法，一般用于呈现连续变量。密度图类似于对直方图进行抽象，用平滑的线来描述数据的分布。这也是一种用来显示数据分布的图表。

```python
data.plot(kind="density", subplots=True, layout=(3, 3), sharex=False)
plt.show()
```

![密度图示例](https://i.imgur.com/tzKijJD.png)

通过密度图来显示数据的分布，相对于直方图更直观。

#### 箱线图
箱线图又称盒须图、盒式图或箱形图，是一种用于显示一组数据分散情况的统计图。因形状如箱子而得名，在各种领域都经常被使用。箱线图也是一种非常好的用于显示数据分布状况的手段。首先画一条中位数线，然后以下四分位数和上四分位数画一个盒子，上下各有一条横线，表示上边缘和下边缘，通过横线来显示数据的伸展状况，游离在边缘之外的点为异常值。

```python
data.plot(kind="box", subplots=True, layout=(3, 3), sharex=False)
plt.show()
```

![箱线图示例](https://i.imgur.com/lS1YdQc.png)

通过图表可以看到不同属性的延伸截然不同。

### 多重图表
多重图表显示不同属性之间的关联关系：相关矩阵图和散点矩阵图。

#### 相关矩阵图
相关矩阵图主要用来展示两个不同属性相互影响的程度。如果两个属性按照相同的方向变化，说明是正向影响。如果两个属性朝相反方向变化，说明是反向影响。把所有属性两两影响的关系展示出来的图表就叫相关矩阵图。矩阵图法就是从多维问题的事件中找出成对的因素，排列成矩阵图，然后根据矩阵图来分析问题，确定关键点。它是一种通过多因素综合思考来探索问题的好方法。

```python
import numpy as np
correlations = data.corr()
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(correlations, vmin=-1, vmax=1)
fig.colorbar(cax)
ticks = np.arange(0, 9, 1)
ax.set_xticks(ticks)
ax.set_yticks(ticks)
ax.set_xticklabels(names)
ax.set_yticklabels(names)
plt.show()
```

![相关矩阵图示例](https://i.imgur.com/pQKKBfC.png)

在图表的左边和上边显示的是完全相同的属性名称，通过这个矩阵可以很清楚地看到各个属性两两关联的关系。

#### 散点矩阵图
散点矩阵图表示因变量随自变量变化的大致趋势，据此可以选择合适的函数对数据点进行拟合。散点矩阵图由两组数据构成多个坐标点，考察坐标点的分布，可以判断两个变量之间是否存在某种关联或总结坐标点的分布模式。散点矩阵图将序列显示为一组点，值由点在图表中的位置表示，类别由图表中的不同标记表示。散点矩阵图通常用于比较跨类别的聚合数据。当同时考察多个变量的相关关系时，若一一绘制它们的简单散点图将十分麻烦。此时可利用散点矩阵图来绘制各个变量间的散点图，这样可以快速发现多个变量间的主要相关性，这在进行多元线性回归时显得尤为重要。

```python
from pandas.plotting import scatter_matrix
scatter_matrix(data)
plt.show()
```

![散点矩阵图示例](https://i.imgur.com/1AtGVRQ.png)

## 总结

#### 数据导入
主要介绍了三种导入 CSV 文件到 Python 的方法，分别是：
- 通过标准的 Python 类库导入
- 通过 NumPy 导入
- 通过 Pandas 导入。
在进行机器学习项目实践时，建议采用 Pandas 方式导入数据。

#### 数据理解
学习了如何分析数据和理解数据。在审查数据的时候，需要记住以下几个小技巧。
- **审查数字**：通常描述性分析给出的数据对数据的理解是不充分的，应该观察思考数据的特点，找到数据的内在联系和对解决问题有什么益处。
- **问为什么**：审查数据后多问几个“为什么”，如你是如何看到和为什么看到这些数字的特殊性的，这些数字和问题如何关联到一起的，以及这些数字和我们的问题有什么关系等。
- **写下想法**：写下自己通过观察得到的想法，用便笺、记事本等将观察到的数据各维度的关联关系和我们的想法都记录下来。例如，数字代表什么，将采用什么样的技术继续挖掘数据等。写下的这些想法将会给新的尝试带来极大的参考价值。

对数据的分析是机器学习中的重要领域，通过对数据的理解，可以选择有效的算法来建立模型。本章介绍了七种方法来观察和理解数据，这为发现数据的特征和选择合适的算法提供了思路。

#### 数据可视化
介绍了几种图表，用于展示数据的分布状况和两两之间的影响关系。结合前一章介绍的方法，我们对数据的理解更加深入了，解决问题的思路也更加清晰了。